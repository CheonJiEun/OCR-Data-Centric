{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# speed_up_loader에 대한 설명"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataloader를 바꿈으로써 학습속도를 높이고자 작성하게 되었습니다.\n",
    "- `do_training`함수를 돌리면서 시간을 측정했습니다.\n",
    "- Dataset은 기존에 주어진 100장의 training image만을 사용하여 시간을 측정했습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 미리보기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original Version\n",
    "    - 현재 사용하고 있는 dataloader의 시간을 측정했습니다.\n",
    "    - 학습 전까지 준비하는 시간 >> **0:00:03.951898**\n",
    "    - 1 epoch 학습하는데 걸리는 시간 >> **0:03:13.311961**\n",
    "- New Version-1\n",
    "    - 이미지를 불러오는 부분 전까지만 미리 진행했을 때 시간을 측정했습니다.\n",
    "    - 학습 전까지 준비하는 시간 >> **0:00:03.740865**\n",
    "    - 1 epoch 학습하는데 걸리는 시간 >> **0:00:32.693058**\n",
    "- New Version-2\n",
    "    - 이미지를 미리 저장해두고 dataloader에서는 transform만 수행되게 됩니다.\n",
    "    - 학습 전까지 준비하는 시간 >> **0:00:03.659511**\n",
    "    - 1 epoch 학습하는데 걸리는 시간 >> **0:00:31.879351**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/opt/ml/input/local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from east_dataset import EASTDataset\n",
    "from model import EAST\n",
    "from dataset import (filter_vertices, cal_distance, move_points, shrink_poly, get_rotate_mat,\n",
    "                     rotate_vertices, get_boundary, cal_error, find_min_rect_angle,\n",
    "                     is_cross_text, crop_img, rotate_all_pixels, resize_img, adjust_height, rotate_img,\n",
    "                     generate_roi_mask, filter_vertices)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import albumentations as A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    # CUDA randomness\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/medical\"\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "num_worekrs = 8\n",
    "image_size = 2048\n",
    "input_size = 1024\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "max_epoch = 1\n",
    "ignore_tags = [\"masked\", \"excluded-region\", \"maintable\", \"stamp\"]\n",
    "seed = 1333"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 사용하고 있는 dataloader의 시간을 측정했습니다.\n",
    "- 학습 전까지 준비하는 시간 >> 0:00:03.951898\n",
    "- 1 epoch 학습하는데 걸리는 시간 >> 0:03:13.311961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        split=\"train\",\n",
    "        image_size=2048,\n",
    "        crop_size=1024,\n",
    "        ignore_tags=[],\n",
    "        ignore_under_threshold=10,\n",
    "        drop_under_threshold=1,\n",
    "        color_jitter=True,\n",
    "        normalize=True,\n",
    "    ):\n",
    "        with open(osp.join(root_dir, \"ufo/{}.json\".format(split)), \"r\") as f:\n",
    "            anno = json.load(f)\n",
    "\n",
    "        self.anno = anno\n",
    "        self.image_fnames = sorted(anno[\"images\"].keys())\n",
    "        self.image_dir = osp.join(root_dir, \"img\", split)\n",
    "\n",
    "        self.image_size, self.crop_size = image_size, crop_size\n",
    "        self.color_jitter, self.normalize = color_jitter, normalize\n",
    "\n",
    "        self.ignore_tags = ignore_tags\n",
    "\n",
    "        self.drop_under_threshold = drop_under_threshold\n",
    "        self.ignore_under_threshold = ignore_under_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_fname = self.image_fnames[idx]\n",
    "        image_fpath = osp.join(self.image_dir, image_fname)\n",
    "\n",
    "        vertices, labels = [], []\n",
    "        for word_info in self.anno[\"images\"][image_fname][\"words\"].values():\n",
    "            word_tags = word_info[\"tags\"]\n",
    "\n",
    "            ignore_sample = any(elem for elem in word_tags if elem in self.ignore_tags)\n",
    "            num_pts = np.array(word_info[\"points\"]).shape[0]\n",
    "\n",
    "            # skip samples with ignore tag and\n",
    "            # samples with number of points greater than 4\n",
    "            if ignore_sample or num_pts > 4:\n",
    "                continue\n",
    "\n",
    "            vertices.append(np.array(word_info[\"points\"]).flatten())\n",
    "            labels.append(int(not word_info[\"illegibility\"]))\n",
    "        vertices, labels = np.array(vertices, dtype=np.float32), np.array(\n",
    "            labels, dtype=np.int64\n",
    "        )\n",
    "\n",
    "        vertices, labels = filter_vertices(\n",
    "            vertices,\n",
    "            labels,\n",
    "            ignore_under=self.ignore_under_threshold,\n",
    "            drop_under=self.drop_under_threshold,\n",
    "        )\n",
    "\n",
    "        image = Image.open(image_fpath)\n",
    "        image, vertices = resize_img(image, vertices, self.image_size)\n",
    "        image, vertices = adjust_height(image, vertices)\n",
    "        image, vertices = rotate_img(image, vertices)\n",
    "        image, vertices = crop_img(image, vertices, labels, self.crop_size)\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        funcs = []\n",
    "        if self.color_jitter:\n",
    "            funcs.append(A.ColorJitter(0.5, 0.5, 0.5, 0.25))\n",
    "        if self.normalize:\n",
    "            funcs.append(A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "        transform = A.Compose(funcs)\n",
    "\n",
    "        image = transform(image=image)[\"image\"]\n",
    "        word_bboxes = np.reshape(vertices, (-1, 4, 2))\n",
    "        roi_mask = generate_roi_mask(image, vertices, labels)\n",
    "\n",
    "        return image, word_bboxes, roi_mask\n",
    "\n",
    "\n",
    "def do_training(\n",
    "    data_dir,\n",
    "    device,\n",
    "    num_workers,\n",
    "    image_size,\n",
    "    input_size,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    max_epoch,\n",
    "    ignore_tags,\n",
    "    seed,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = SceneTextDataset(\n",
    "        data_dir,\n",
    "        split=\"train_default\",\n",
    "        image_size=image_size,\n",
    "        crop_size=input_size,\n",
    "        ignore_tags=ignore_tags,\n",
    "    )\n",
    "    dataset = EASTDataset(dataset)\n",
    "    num_batches = math.ceil(len(dataset) / batch_size)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[max_epoch // 2], gamma=0.1\n",
    "    )\n",
    "    print(f\"학습 전까지 준비하는 시간 >> {timedelta(seconds=time.time() - start_time)}\")\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(max_epoch):\n",
    "        epoch_start = time.time()\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            for img, gt_score_map, gt_geo_map, roi_mask in train_loader:\n",
    "                continue\n",
    "    print(f\"1 epoch 학습하는데 걸리는 시간 >> {timedelta(seconds=time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전까지 준비하는 시간 >> 0:00:03.951898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [03:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch 학습하는데 걸리는 시간 >> 0:03:13.311961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_training(data_dir, \n",
    "            device, \n",
    "            num_worekrs, \n",
    "            image_size, \n",
    "            input_size, \n",
    "            batch_size, \n",
    "            learning_rate, \n",
    "            max_epoch, \n",
    "            ignore_tags, \n",
    "            seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Version-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지를 불러오는 부분 전까지만 미리 진행했을 때 시간을 측정했습니다.\n",
    "- 학습 전까지 준비하는 시간 >> 0:00:03.740865\n",
    "- 1 epoch 학습하는데 걸리는 시간 >> 0:00:32.693058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        split=\"train\",\n",
    "        image_size=2048,\n",
    "        crop_size=1024,\n",
    "        ignore_tags=[],\n",
    "        ignore_under_threshold=10,\n",
    "        drop_under_threshold=1,\n",
    "        color_jitter=True,\n",
    "        normalize=True,\n",
    "    ):\n",
    "        with open(osp.join(root_dir, \"ufo/{}.json\".format(split)), \"r\") as f:\n",
    "            anno = json.load(f)\n",
    "\n",
    "        self.anno = anno\n",
    "        self.image_fnames = sorted(anno[\"images\"].keys())[:3]\n",
    "        self.image_dir = osp.join(root_dir, \"img\", split)\n",
    "\n",
    "        self.image_size, self.crop_size = image_size, crop_size\n",
    "        self.color_jitter, self.normalize = color_jitter, normalize\n",
    "\n",
    "        self.ignore_tags = ignore_tags\n",
    "\n",
    "        self.drop_under_threshold = drop_under_threshold\n",
    "        self.ignore_under_threshold = ignore_under_threshold\n",
    "\n",
    "        self.images = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        for idx in range(len(self.image_fnames)):\n",
    "            image_fname = self.image_fnames[idx]\n",
    "            image_fpath = osp.join(self.image_dir, image_fname)\n",
    "            vertices, labels = [], []\n",
    "            for word_info in self.anno[\"images\"][image_fname][\"words\"].values():\n",
    "                word_tags = word_info[\"tags\"]\n",
    "                ignore_sample = any(elem for elem in word_tags if elem in self.ignore_tags)\n",
    "                num_pts = np.array(word_info[\"points\"]).shape[0]\n",
    "\n",
    "                if ignore_sample or num_pts > 4:\n",
    "                    continue\n",
    "                vertices.append(np.array(word_info[\"points\"]).flatten())\n",
    "                labels.append(int(not word_info[\"illegibility\"]))\n",
    "            vertices, labels = np.array(vertices, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "            vertices, labels = filter_vertices(vertices, labels, \n",
    "                                               ignore_under=self.ignore_under_threshold, \n",
    "                                               drop_under=self.drop_under_threshold)\n",
    "            self.images.append(image_fpath)\n",
    "            self.vertices.append(vertices)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_fpath = self.images[idx]\n",
    "        vertices = self.vertices[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        image = Image.open(image_fpath)\n",
    "        image, vertices = resize_img(image, vertices, self.image_size)\n",
    "        image, vertices = adjust_height(image, vertices)\n",
    "        image, vertices = rotate_img(image, vertices)\n",
    "        image, vertices = crop_img(image, vertices, labels, self.crop_size)\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        funcs = []\n",
    "        if self.color_jitter:\n",
    "            funcs.append(A.ColorJitter(0.5, 0.5, 0.5, 0.25))\n",
    "        if self.normalize:\n",
    "            funcs.append(A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "        transform = A.Compose(funcs)\n",
    "\n",
    "        image = transform(image=image)[\"image\"]\n",
    "        word_bboxes = np.reshape(vertices, (-1, 4, 2))\n",
    "        roi_mask = generate_roi_mask(image, vertices, labels)\n",
    "\n",
    "        return image, word_bboxes, roi_mask\n",
    "\n",
    "\n",
    "def do_training(\n",
    "    data_dir,\n",
    "    device,\n",
    "    num_workers,\n",
    "    image_size,\n",
    "    input_size,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    max_epoch,\n",
    "    ignore_tags,\n",
    "    seed,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = SceneTextDataset(\n",
    "        data_dir,\n",
    "        split=\"train_default\",\n",
    "        image_size=image_size,\n",
    "        crop_size=input_size,\n",
    "        ignore_tags=ignore_tags,\n",
    "    )\n",
    "    dataset = EASTDataset(dataset)\n",
    "    num_batches = math.ceil(len(dataset) / batch_size)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[max_epoch // 2], gamma=0.1\n",
    "    )\n",
    "    print(f\"학습 전까지 준비하는 시간 >> {timedelta(seconds=time.time() - start_time)}\")\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(max_epoch):\n",
    "        epoch_start = time.time()\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            for img, gt_score_map, gt_geo_map, roi_mask in train_loader:\n",
    "                continue\n",
    "    print(f\"1 epoch 학습하는데 걸리는 시간 >> {timedelta(seconds=time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전까지 준비하는 시간 >> 0:00:03.740865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch 학습하는데 걸리는 시간 >> 0:00:32.693058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_training(data_dir, \n",
    "            device, \n",
    "            num_worekrs, \n",
    "            image_size, \n",
    "            input_size, \n",
    "            batch_size, \n",
    "            learning_rate, \n",
    "            max_epoch, \n",
    "            ignore_tags, \n",
    "            seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Version-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지를 미리 저장해두고 dataloader에서는 transform만 수행되게 됩니다.\n",
    "- 학습 전까지 준비하는 시간 >> 0:00:03.659511\n",
    "- 1 epoch 학습하는데 걸리는 시간 >> 0:00:31.879351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        split=\"train\",\n",
    "        image_size=2048,\n",
    "        crop_size=1024,\n",
    "        ignore_tags=[],\n",
    "        ignore_under_threshold=10,\n",
    "        drop_under_threshold=1,\n",
    "        color_jitter=True,\n",
    "        normalize=True,\n",
    "    ):\n",
    "        with open(osp.join(root_dir, \"ufo/{}.json\".format(split)), \"r\") as f:\n",
    "            anno = json.load(f)\n",
    "\n",
    "        self.anno = anno\n",
    "        self.image_fnames = sorted(anno[\"images\"].keys())[:3]\n",
    "        self.image_dir = osp.join(root_dir, \"img\", split)\n",
    "\n",
    "        self.image_size, self.crop_size = image_size, crop_size\n",
    "        self.color_jitter, self.normalize = color_jitter, normalize\n",
    "\n",
    "        self.ignore_tags = ignore_tags\n",
    "\n",
    "        self.drop_under_threshold = drop_under_threshold\n",
    "        self.ignore_under_threshold = ignore_under_threshold\n",
    "\n",
    "        self.images = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        for idx in range(len(self.image_fnames)):\n",
    "            image_fname = self.image_fnames[idx]\n",
    "            image_fpath = osp.join(self.image_dir, image_fname)\n",
    "            vertices, labels = [], []\n",
    "            for word_info in self.anno[\"images\"][image_fname][\"words\"].values():\n",
    "                word_tags = word_info[\"tags\"]\n",
    "                ignore_sample = any(elem for elem in word_tags if elem in self.ignore_tags)\n",
    "                num_pts = np.array(word_info[\"points\"]).shape[0]\n",
    "\n",
    "                if ignore_sample or num_pts > 4:\n",
    "                    continue\n",
    "                vertices.append(np.array(word_info[\"points\"]).flatten())\n",
    "                labels.append(int(not word_info[\"illegibility\"]))\n",
    "            vertices, labels = np.array(vertices, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "            vertices, labels = filter_vertices(vertices, labels, ignore_under=self.ignore_under_threshold, drop_under=self.drop_under_threshold)\n",
    "            self.images.append(Image.open(image_fpath))\n",
    "            self.vertices.append(vertices)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        vertices = self.vertices[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        image, vertices = resize_img(image, vertices, self.image_size)\n",
    "        image, vertices = adjust_height(image, vertices)\n",
    "        image, vertices = rotate_img(image, vertices)\n",
    "        image, vertices = crop_img(image, vertices, labels, self.crop_size)\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        funcs = []\n",
    "        if self.color_jitter:\n",
    "            funcs.append(A.ColorJitter(0.5, 0.5, 0.5, 0.25))\n",
    "        if self.normalize:\n",
    "            funcs.append(A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "        transform = A.Compose(funcs)\n",
    "\n",
    "        image = transform(image=image)[\"image\"]\n",
    "        word_bboxes = np.reshape(vertices, (-1, 4, 2))\n",
    "        roi_mask = generate_roi_mask(image, vertices, labels)\n",
    "\n",
    "        return image, word_bboxes, roi_mask\n",
    "\n",
    "\n",
    "def do_training(\n",
    "    data_dir,\n",
    "    device,\n",
    "    num_workers,\n",
    "    image_size,\n",
    "    input_size,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    max_epoch,\n",
    "    ignore_tags,\n",
    "    seed,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = SceneTextDataset(\n",
    "        data_dir,\n",
    "        split=\"train_default\",\n",
    "        image_size=image_size,\n",
    "        crop_size=input_size,\n",
    "        ignore_tags=ignore_tags,\n",
    "    )\n",
    "    dataset = EASTDataset(dataset)\n",
    "    num_batches = math.ceil(len(dataset) / batch_size)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[max_epoch // 2], gamma=0.1\n",
    "    )\n",
    "    print(f\"학습 전까지 준비하는 시간 >> {timedelta(seconds=time.time() - start_time)}\")\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(max_epoch):\n",
    "        epoch_start = time.time()\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            for img, gt_score_map, gt_geo_map, roi_mask in train_loader:\n",
    "                continue\n",
    "    print(f\"1 epoch 학습하는데 걸리는 시간 >> {timedelta(seconds=time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전까지 준비하는 시간 >> 0:00:03.659511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch 학습하는데 걸리는 시간 >> 0:00:31.879351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_training(data_dir, \n",
    "            device, \n",
    "            num_worekrs, \n",
    "            image_size, \n",
    "            input_size, \n",
    "            batch_size, \n",
    "            learning_rate, \n",
    "            max_epoch, \n",
    "            ignore_tags, \n",
    "            seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
